import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from operator import itemgetter
from pymongo import MongoClient
import os
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="MongoDB RAG Chat Interface",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = ChatMessageHistory()
if 'messages' not in st.session_state:
    st.session_state.messages = []

# MongoDB ë° RAG ì´ˆê¸°í™”
@st.cache_resource
def initialize_rag():
    load_dotenv()
    
    try:
        # MongoDB ì—°ê²° ì„¤ì •
        mongo_path = os.getenv("MONGO_PATH")
        logger.info("Attempting to connect to MongoDB...")
        
        mongo_client = MongoClient(mongo_path)
        db = mongo_client["notice-db"]
        collection = db["test_embedded"]
        
        # MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ ë° ë°ì´í„° í™•ì¸
        try:
            mongo_client.admin.command('ping')
            doc_count = collection.count_documents({})
            logger.info(f"MongoDB ì—°ê²° ì„±ê³µ! ì „ì²´ ë¬¸ì„œ ìˆ˜: {doc_count}")
            
            # ìƒ˜í”Œ ë¬¸ì„œ í™•ì¸
            sample_doc = collection.find_one()
            if sample_doc:
                logger.info(f"ìƒ˜í”Œ ë¬¸ì„œ í•„ë“œ: {list(sample_doc.keys())}")
            else:
                logger.warning("ì»¬ë ‰ì…˜ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
        except Exception as e:
            logger.error(f"MongoDB ì—°ê²°/ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
        
        # OpenAI Embeddings ì´ˆê¸°í™”
        embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
        logger.info("OpenAI embeddings initialized")
        
        # Vector Store ì´ˆê¸°í™”
        vector_store = MongoDBAtlasVectorSearch(
            collection=collection,
            embedding=embeddings,
            index_name="vector_index",
            text_key="context"
        )
        logger.info("Vector store initialized")
        
        # Retriever ì„¤ì • ë° í…ŒìŠ¤íŠ¸
        retriever = vector_store.as_retriever()
        logger.info("Retriever ì„¤ì • ì™„ë£Œ")
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        try:
            test_results = retriever.get_relevant_documents("test")
            logger.info(f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {len(test_results)} ë¬¸ì„œ ê²€ìƒ‰ë¨")
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt = ChatPromptTemplate.from_messages([
            (
                "system",
                """You are an AI assistant specializing in Question-Answering (QA) tasks within a Retrieval-Augmented Generation (RAG) system. 
Our primary mission is to answer questions based on provided context or chat history.
Ensure your response is concise and directly addresses the question without any additional narration.
###
You may consider the previous conversation history to answer the question.
# Here's the previous conversation history:
{chat_history}
###
Your final answer should be written concisely (but include important numerical values, technical terms, jargon, and names), followed by the source of the information.
# Steps
1. Carefully read and understand the context provided.
2. Identify the key information related to the question within the context.
3. Formulate a concise answer based on the relevant information.
4. Ensure your final answer directly addresses the question.
5. List the source of the answer in bullet points, which must be a file name (with a page number) or URL from the context. Omit if the answer is based on previous conversation or if the source cannot be found.
# Output Format:
Your final answer here, with numerical values, technical terms, jargon, and names in their original language
**Source**(Optional)
(Source of the answer, must be a file name(with a page number) or URL from the context. Omit if the answer is based on previous conversation or can't find the source.)
(list more if there are multiple sources)
...
###
Remember:
It's crucial to base your answer solely on the **provided context** or **chat history**. 
DO NOT use any external knowledge or information not present in the given materials.
If a user asks based on the previous conversation, but if there's no previous conversation or not enough information, you should answer that you don't know.
###
# Here is the user's question:
{question}
# Here is the context that you should use to answer the question:
{context}
# Your final answer to the user's question:"""
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
            ("assistant", "{context}")
        ])
        
        # LLM ì„¤ì •
        llm = ChatOpenAI(model_name="gpt-4", temperature=0)
        
        # RAG ì²´ì¸ ìƒì„±
        rag_chain = (
            {
                "context": itemgetter("question") | retriever,
                "question": itemgetter("question"),
                "chat_history": itemgetter("chat_history"),
            }
            | prompt
            | llm
        )
        
        return rag_chain
        
    except Exception as e:
        logger.error(f"Error during initialization: {str(e)}", exc_info=True)
        raise

# UI êµ¬ì„±
st.title("MongoDB RAG Chat Interface ğŸ¤–")
st.markdown("---")

# ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
chat_container = st.container()
with chat_container:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
prompt = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
if prompt:
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    try:
        # RAG ì²´ì¸ ì‹¤í–‰
        rag_chain = initialize_rag()
        
        # ì„¸ì…˜ ê¸°ë¡ ê´€ë¦¬
        store = {
            "default": st.session_state.chat_history
        }
        
        def get_session_history(session_id):
            return store[session_id]
        
        chain_with_history = RunnableWithMessageHistory(
            rag_chain,
            get_session_history,
            input_messages_key="question",
            history_messages_key="chat_history",
        )
        
        # ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                response = chain_with_history.invoke(
                    {"question": prompt},
                    config={"configurable": {"session_id": "default"}},
                )
                st.markdown(response.content)
        
        # ì‘ë‹µ ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": response.content})
        
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ì„¤ì •")
    if st.button("ëŒ€í™” ë‚´ì—­ ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.session_state.chat_history = ChatMessageHistory()
        st.experimental_rerun()
    
    st.markdown("---")
    st.markdown("### ì‹œìŠ¤í…œ ì •ë³´")
    st.info("MongoDB RAG ì‹œìŠ¤í…œì´ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    # ì•±ì´ ì²˜ìŒ ë¡œë“œë  ë•Œ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    initialize_rag()