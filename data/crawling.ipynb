{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\82104\\anaconda3\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from selenium) (0.23.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\82104\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pyder (c:\\users\\82104\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pyder (c:\\users\\82104\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver_manager in c:\\users\\82104\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\82104\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\82104\\anaconda3\\lib\\site-packages (from webdriver_manager) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\82104\\anaconda3\\lib\\site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.6)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pyder (c:\\users\\82104\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pyder (c:\\users\\82104\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver_manager --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '문과대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 문과대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://libart.yonsei.ac.kr/libart/board/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도 맞춤 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '문과대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <dd> 태그에서 텍스트를 추출\n",
    "    context_tag = soup.select_one('dd .fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "    time.sleep(1)  # 서버 부하 방지를 위한 대기 시간 설정\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"문과대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://libart.yonsei.ac.kr/libart/board/notice.do?mode=view&articleNo=213970&article.offset=0&articleLimit=10'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['link'][0]\n",
    "# df['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '상경대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 상경대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://yce.yonsei.ac.kr/ybe/notice/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '상경대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <dd> 태그에서 텍스트를 추출\n",
    "    context_tag = soup.select_one('dd .fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "    time.sleep(1)  # 서버 부하 방지를 위한 대기 시간 설정\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"상경대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '경영대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 경영대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://ysb.yonsei.ac.kr/board.asp\"\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    # 페이지 번호를 포함한 요청 파라미터 설정\n",
    "    params = {\n",
    "        'mid': 'm06_01',\n",
    "        'page': page\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # 인코딩 문제 해결 (응답의 인코딩을 명시적으로 설정)\n",
    "    response.encoding = 'euc-kr'  # utf-8로 하면 한글이 깨짐\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a[href*=\"board.asp?act=view\"]')  # 제목 추출\n",
    "        date_tag = row.select_one('td.board_date')  # 작성일 추출\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = date_tag.get_text(strip=True)  # 작성일 추출\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '경영대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.encoding = 'euc-kr'  # 인코딩 설정\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 #BoardContent 태그에서 텍스트를 추출\n",
    "    context_tag = soup.select_one('#BoardContent')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "    time.sleep(1)  # 서버 부하 방지를 위한 대기 시간 설정\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"경영대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '공과대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 공과대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://engineering.yonsei.ac.kr/engineering/board/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):  # 각 공지사항이 있는 행 선택\n",
    "        title_tag = row.select_one('a.c-board-title')\n",
    "        td_tags = row.select('td')  # <td> 태그 모두 선택\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:  # <td> 태그가 존재하는지 확인\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()  # 공지 태그 제거\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "\n",
    "            # 작성일은 마지막 <td> 태그에서 추출\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '공과대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "    time.sleep(1)  # 서버 부하 방지를 위한 대기 시간 설정\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"공과대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '이과대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 이과대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"http://science.yonsei.ac.kr\"\n",
    "notice_url = f\"{base_url}/community/notice\"\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page_number):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    url = f\"{notice_url}?p={page_number}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('td.nxb-list-table__title a')\n",
    "        date_tag = row.select_one('td.nxb-list-table__date')\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = date_tag.get_text(strip=True)\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '이과대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"editor-contents\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.editor-contents')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 원하는 페이지 범위 설정 (1페이지부터 5페이지까지 크롤링)\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"이과대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '생명시스템대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 생명시스템대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://bio.yonsei.ac.kr/bio/board/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '생명시스템대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"생명시스템대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '인공지능융합대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 인공지능융합대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://computing.yonsei.ac.kr/bbs/board.php\"\n",
    "params = {\n",
    "    'bo_table': 'sub4_4',\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['page'] = page  # 페이지 번호 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('div.bo_tit a')  # 제목 추출\n",
    "        date_tag = row.select_one('td.td_datetime')  # 작성일 추출\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)  # 제목 정제\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = date_tag.get_text(strip=True)  # 작성일 추출\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '인공지능융합대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div id=\"bo_v_con\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div#bo_v_con')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"인공지능융합대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '신과대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 신과대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://yonshin.yonsei.ac.kr/theology/board02/undergraduate.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '신과대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"신과대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '사회과학대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 사회과학대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://socsci.yonsei.ac.kr/socsci/community/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '사회과학대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"사회과학대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '음악대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 음악대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://music.yonsei.ac.kr/music/notice1.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '음악대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"음악대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '생활과학대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 생활과학대학핑\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://che.yonsei.ac.kr/che/community_che/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = base_url + title_tag['href']  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도 맞춤 작성일\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '생활과학대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link\n",
    "            })\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"생활과학대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '생활과학대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://che.yonsei.ac.kr/che/community_che/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도 맞춤 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '생활과학대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"생활과학대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '교육과학대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 교육과학대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://ysces.yonsei.ac.kr/yses/edu/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        date_tag = row.select_one('td')  # 작성일 추출\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + date_tag.get_text(strip=True)  # 연도 맞춤 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '교육과학대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"교육과학대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '학부대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 학부대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://universitycollege.yonsei.ac.kr/fresh/infomation/student.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('td.text-left'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        date_tag = row.select_one('div.c-board-info-m > span:nth-child(2)')  # 작성일 추출\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = date_tag.get_text(strip=True)  # 작성일 그대로 가져옴\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '학부대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"학부대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '글로벌인재대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 글로벌인재대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://glc.yonsei.ac.kr/notice/\"\n",
    "params = {\n",
    "    'mod': 'list',\n",
    "    'pageid': 1  # 페이지 번호를 설정할 때 사용\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page_id):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['pageid'] = page_id  # 페이지 번호 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    rows = soup.select('div.kboard-default-cut-strings')\n",
    "    dates = [tag for tag in soup.select('td.kboard-list-date') if '작성일' not in tag.get_text()]\n",
    "\n",
    "    for row, date_tag in zip(rows, dates):\n",
    "        title = row.get_text(strip=True)  # 제목 추출\n",
    "        link_tag = row.find_parent(\"a\")  # 링크가 포함된 부모 태그 찾기\n",
    "        link = urljoin(base_url, link_tag['href']) if link_tag else None  # 링크가 있는 경우 절대 경로로 변환\n",
    "        date = date_tag.get_text(strip=True)  # 작성일 추출\n",
    "\n",
    "        # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "        context = scrape_notice_context(link) if link else \"내용 없음\"\n",
    "\n",
    "        all_notices.append({\n",
    "            'department': '글로벌인재대학',\n",
    "            'title': title,\n",
    "            'date': date,\n",
    "            'link': link,\n",
    "            'context': context\n",
    "        })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"content-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.content-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(1, 6):  # 페이지 번호는 1부터 시작\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"글로벌인재대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '국어국문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 국어국문학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://koreanlit.yonsei.ac.kr/korean/notice01.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')\n",
    "        \n",
    "        # `td` 요소가 있고 마지막 `td`가 작성일을 포함하는지 확인\n",
    "        if title_tag and td_tags and len(td_tags) > 1:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = td_tags[-1].get_text(strip=True)  # 마지막 `td` 요소가 작성일\n",
    "            \n",
    "            # 날짜에 '20'을 붙여 '2024.10.14' 형식으로 저장\n",
    "            date = f\"20{raw_date}\" if raw_date else \"날짜 없음\"\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '국어국문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"국어국문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '중어중문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 중어중문학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://ycll.yonsei.ac.kr/yonseicll/board01.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        date_tags = row.select('td')  # <td> 요소 전체를 선택\n",
    "\n",
    "        # <td> 요소가 존재하고, 날짜가 포함된 마지막 <td> 요소가 있는지 확인\n",
    "        if title_tag and date_tags:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = date_tags[-1].get_text(strip=True)\n",
    "            \n",
    "            # '24.10.31' 형식을 '2024.10.31'로 변환\n",
    "            date = f\"20{raw_date}\" if len(raw_date) == 8 else raw_date\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '중어중문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"중어중문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '영어영문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 영어영문학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://ysenglish.yonsei.ac.kr/ysenglish/process/process.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "        \n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '영어영문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"영어영문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '독어독문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 독어독문학과과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://ysgerman.yonsei.ac.kr/ysgerman/ger_notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '독어독문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"독어독문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '불어불문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 불어불문학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://franys.yonsei.ac.kr/franys/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '불어불문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"불어불문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '노어노문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 노어노문학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://rus.yonsei.ac.kr/russia/borad_main.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            # 제목 텍스트를 불필요한 문자열을 제거하며 추출\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").replace(\"[학부]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '노어노문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"노어노문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '사학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 사학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://history.yonsei.ac.kr/histroy/02_announce/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            # 제목 텍스트를 불필요한 문자열을 제거하며 추출\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '사학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"사학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '철학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 철학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://philosophy.yonsei.ac.kr/cholhak/process/process.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '철학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"철학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '문헌정보학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 문헌정보학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://yslis.yonsei.ac.kr/doai/board/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '문헌정보학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"문헌정보학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '경제학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 경제학과\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import urllib3\n",
    "\n",
    "# SSL 경고 무시 설정\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://econ.yonsei.ac.kr/econ/board/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params, verify=False)  # SSL 검증 비활성화\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '경제학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link, verify=False)  # SSL 검증 비활성화\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"경제학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '응용통계학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 응용통계학과\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://stat.yonsei.ac.kr/stat/board/under_notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# List to store all notices\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"Scrapes a page of notices based on the offset.\"\"\"\n",
    "    params['article.offset'] = offset  # Set page offset\n",
    "    response = requests.get(base_url, params=params, verify=False)  # Disable SSL verification\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all notices on the page\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')\n",
    "        tds = row.select('td')\n",
    "\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            raw_date = tds[-1].get_text(strip=True)\n",
    "\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # Convert to \"YYYY.MM.DD\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '응용통계학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"Scrapes the content of an individual notice.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Scraping the first 5 pages\n",
    "for page in range(5):\n",
    "    offset = page * 10\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# Remove duplicates based on title and date\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"응용통계학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '수학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 수학과\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://math.yonsei.ac.kr/math/math/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# List to store all notices\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"Scrapes a page of notices based on the offset.\"\"\"\n",
    "    params['article.offset'] = offset  # Set page offset\n",
    "    response = requests.get(base_url, params=params, verify=False)  # Disable SSL verification\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all notices on the page\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')\n",
    "        tds = row.select('td')\n",
    "\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            raw_date = tds[-1].get_text(strip=True)\n",
    "\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # Convert to \"YYYY.MM.DD\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '수학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"Scrapes the content of an individual notice.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Scraping the first 5 pages\n",
    "for page in range(5):\n",
    "    offset = page * 10\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# Remove duplicates based on title and date\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"수학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '물리학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 물리학과\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://physics.yonsei.ac.kr/notice/board\"\n",
    "params = {\n",
    "    'page': 1\n",
    "}\n",
    "\n",
    "# List to store all notices\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"Scrapes a page of notices based on the page number.\"\"\"\n",
    "    params['page'] = page  # Set page number\n",
    "    response = requests.get(base_url, params=params, verify=False)  # Disable SSL verification\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all rows containing notices\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('td.td-subject a')\n",
    "        date_tag = row.select_one('td.td-date')\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            date = date_tag.get_text(strip=True)\n",
    "\n",
    "            # Get context from the detailed page\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '물리학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"Scrapes the content of an individual notice.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract context from the notice content\n",
    "    context_tag = soup.select_one('div.bw_contents.editor_contents')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Scraping the first 5 pages\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# Remove duplicates based on title and date\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# Check if the dataframe is not empty\n",
    "if not df.empty:\n",
    "    # Save to CSV\n",
    "    csv_filename = \"물리학과_공지사항.csv\"\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n",
    "else:\n",
    "    print(\"데이터프레임이 비어 있습니다. HTML 구조를 확인해주세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '화학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 화학과\n",
    "\n",
    "# SSL 경고 비활성화\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://chemyonsei.kr/board/notice\"\n",
    "params = {\n",
    "    'p': 1\n",
    "}\n",
    "\n",
    "# 전체 공지사항 저장 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"해당 페이지의 공지사항 목록을 크롤링합니다.\"\"\"\n",
    "    params['p'] = page  # 페이지 번호 설정\n",
    "    response = requests.get(base_url, params=params, verify=False)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 공지사항 리스트에서 제목과 날짜를 찾습니다.\n",
    "    for row in soup.select('a.text-small'):\n",
    "        title_tag = row\n",
    "        date_tag = row.find_next('time')\n",
    "        \n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            date = date_tag.get_text(strip=True)\n",
    "\n",
    "            # 공지사항의 상세 내용을 크롤링합니다.\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '화학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"개별 공지사항의 상세 내용을 크롤링합니다.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용 크롤링\n",
    "    context_tag = soup.select_one('div.editor-contents')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 첫 5페이지 크롤링\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성 및 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"화학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "CSV 파일이 '지구시스템과학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 지구시스템과학과\n",
    "\n",
    "# SSL 경고 비활성화\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"http://geo.yonsei.ac.kr/index.php\"\n",
    "params = {\n",
    "    'hCode': 'BOARD',\n",
    "    'bo_idx': 2,\n",
    "    'page': 'list',\n",
    "    'pg': 1\n",
    "}\n",
    "\n",
    "# 전체 공지사항 저장 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"해당 페이지의 공지사항 목록을 크롤링합니다.\"\"\"\n",
    "    params['pg'] = page  # 페이지 번호 설정\n",
    "    response = requests.get(base_url, params=params, verify=False)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 공지사항 리스트에서 제목과 날짜를 찾습니다.\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a[href^=\"?page=view\"]')\n",
    "        date_tag = row.select_one('td:nth-last-child(1)')  # 날짜가 마지막 열에 있다고 가정\n",
    "        \n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            date = date_tag.get_text(strip=True)\n",
    "\n",
    "            # 공지사항의 상세 내용을 크롤링합니다.\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '지구시스템과학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"개별 공지사항의 상세 내용을 크롤링합니다.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용 크롤링\n",
    "    context_tag = soup.select_one('div.board_content')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 첫 5페이지 크롤링\n",
    "for page in range(1, 3):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성 및 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"지구시스템과학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
