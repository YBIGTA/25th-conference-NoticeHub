{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\82104\\anaconda3\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from selenium) (0.23.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pyder (c:\\users\\82104\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pyder (c:\\users\\82104\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pycparser in c:\\users\\82104\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver_manager in c:\\users\\82104\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\82104\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\82104\\anaconda3\\lib\\site-packages (from webdriver_manager) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\82104\\anaconda3\\lib\\site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.3.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pyder (c:\\users\\82104\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pyder (c:\\users\\82104\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82104\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver_manager --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '문과대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 문과대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://libart.yonsei.ac.kr/libart/board/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도 맞춤 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '문과대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <dd> 태그에서 텍스트를 추출\n",
    "    context_tag = soup.select_one('dd .fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "    time.sleep(1)  # 서버 부하 방지를 위한 대기 시간 설정\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"문과대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://libart.yonsei.ac.kr/libart/board/notice.do?mode=view&articleNo=213970&article.offset=0&articleLimit=10'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['link'][0]\n",
    "# df['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '상경대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 상경대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://yce.yonsei.ac.kr/ybe/notice/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '상경대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <dd> 태그에서 텍스트를 추출\n",
    "    context_tag = soup.select_one('dd .fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "    time.sleep(1)  # 서버 부하 방지를 위한 대기 시간 설정\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"상경대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '경영대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 경영대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://ysb.yonsei.ac.kr/board.asp\"\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    # 페이지 번호를 포함한 요청 파라미터 설정\n",
    "    params = {\n",
    "        'mid': 'm06_01',\n",
    "        'page': page\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # 인코딩 문제 해결 (응답의 인코딩을 명시적으로 설정)\n",
    "    response.encoding = 'euc-kr'  # utf-8로 하면 한글이 깨짐\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a[href*=\"board.asp?act=view\"]')  # 제목 추출\n",
    "        date_tag = row.select_one('td.board_date')  # 작성일 추출\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = date_tag.get_text(strip=True)  # 작성일 추출\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '경영대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.encoding = 'euc-kr'  # 인코딩 설정\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 #BoardContent 태그에서 텍스트를 추출\n",
    "    context_tag = soup.select_one('#BoardContent')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "    time.sleep(1)  # 서버 부하 방지를 위한 대기 시간 설정\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"경영대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '공과대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 공과대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://engineering.yonsei.ac.kr/engineering/board/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):  # 각 공지사항이 있는 행 선택\n",
    "        title_tag = row.select_one('a.c-board-title')\n",
    "        td_tags = row.select('td')  # <td> 태그 모두 선택\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:  # <td> 태그가 존재하는지 확인\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()  # 공지 태그 제거\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "\n",
    "            # 작성일은 마지막 <td> 태그에서 추출\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '공과대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "    time.sleep(1)  # 서버 부하 방지를 위한 대기 시간 설정\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"공과대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '이과대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 이과대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"http://science.yonsei.ac.kr\"\n",
    "notice_url = f\"{base_url}/community/notice\"\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page_number):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    url = f\"{notice_url}?p={page_number}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('td.nxb-list-table__title a')\n",
    "        date_tag = row.select_one('td.nxb-list-table__date')\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = date_tag.get_text(strip=True)\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '이과대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"editor-contents\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.editor-contents')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 원하는 페이지 범위 설정 (1페이지부터 5페이지까지 크롤링)\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"이과대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '생명시스템대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 생명시스템대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://bio.yonsei.ac.kr/bio/board/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '생명시스템대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"생명시스템대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '인공지능융합대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 인공지능융합대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://computing.yonsei.ac.kr/bbs/board.php\"\n",
    "params = {\n",
    "    'bo_table': 'sub4_4',\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['page'] = page  # 페이지 번호 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('div.bo_tit a')  # 제목 추출\n",
    "        date_tag = row.select_one('td.td_datetime')  # 작성일 추출\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)  # 제목 정제\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = date_tag.get_text(strip=True)  # 작성일 추출\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '인공지능융합대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div id=\"bo_v_con\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div#bo_v_con')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"인공지능융합대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '신과대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 신과대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://yonshin.yonsei.ac.kr/theology/board02/undergraduate.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '신과대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"신과대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '사회과학대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 사회과학대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://socsci.yonsei.ac.kr/socsci/community/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '사회과학대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"사회과학대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '음악대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 음악대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://music.yonsei.ac.kr/music/notice1.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도를 맞춘 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '음악대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"음악대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '생활과학대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 생활과학대학핑\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://che.yonsei.ac.kr/che/community_che/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = base_url + title_tag['href']  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도 맞춤 작성일\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '생활과학대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link\n",
    "            })\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"생활과학대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '생활과학대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://che.yonsei.ac.kr/che/community_che/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')  # 모든 <td> 태그 추출\n",
    "\n",
    "        if title_tag and len(td_tags) > 0:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + td_tags[-1].get_text(strip=True)  # 연도 맞춤 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '생활과학대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"생활과학대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '교육과학대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 교육과학대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://ysces.yonsei.ac.kr/yses/edu/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        date_tag = row.select_one('td')  # 작성일 추출\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = \"20\" + date_tag.get_text(strip=True)  # 연도 맞춤 작성일\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '교육과학대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"교육과학대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '학부대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 학부대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://universitycollege.yonsei.ac.kr/fresh/infomation/student.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('td.text-left'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        date_tag = row.select_one('div.c-board-info-m > span:nth-child(2)')  # 작성일 추출\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            date = date_tag.get_text(strip=True)  # 작성일 그대로 가져옴\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '학부대학',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"학부대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '글로벌인재대학_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 글로벌인재대학\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://glc.yonsei.ac.kr/notice/\"\n",
    "params = {\n",
    "    'mod': 'list',\n",
    "    'pageid': 1  # 페이지 번호를 설정할 때 사용\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page_id):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['pageid'] = page_id  # 페이지 번호 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    rows = soup.select('div.kboard-default-cut-strings')\n",
    "    dates = [tag for tag in soup.select('td.kboard-list-date') if '작성일' not in tag.get_text()]\n",
    "\n",
    "    for row, date_tag in zip(rows, dates):\n",
    "        title = row.get_text(strip=True)  # 제목 추출\n",
    "        link_tag = row.find_parent(\"a\")  # 링크가 포함된 부모 태그 찾기\n",
    "        link = urljoin(base_url, link_tag['href']) if link_tag else None  # 링크가 있는 경우 절대 경로로 변환\n",
    "        date = date_tag.get_text(strip=True)  # 작성일 추출\n",
    "\n",
    "        # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "        context = scrape_notice_context(link) if link else \"내용 없음\"\n",
    "\n",
    "        all_notices.append({\n",
    "            'department': '글로벌인재대학',\n",
    "            'title': title,\n",
    "            'date': date,\n",
    "            'link': link,\n",
    "            'context': context\n",
    "        })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"content-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.content-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(1, 6):  # 페이지 번호는 1부터 시작\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"글로벌인재대학_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '국어국문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 국어국문학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://koreanlit.yonsei.ac.kr/korean/notice01.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        td_tags = row.select('td')\n",
    "        \n",
    "        # `td` 요소가 있고 마지막 `td`가 작성일을 포함하는지 확인\n",
    "        if title_tag and td_tags and len(td_tags) > 1:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = td_tags[-1].get_text(strip=True)  # 마지막 `td` 요소가 작성일\n",
    "            \n",
    "            # 날짜에 '20'을 붙여 '2024.10.14' 형식으로 저장\n",
    "            date = f\"20{raw_date}\" if raw_date else \"날짜 없음\"\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '국어국문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('dd > div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"국어국문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '중어중문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 중어중문학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://ycll.yonsei.ac.kr/yonseicll/board01.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        date_tags = row.select('td')  # <td> 요소 전체를 선택\n",
    "\n",
    "        # <td> 요소가 존재하고, 날짜가 포함된 마지막 <td> 요소가 있는지 확인\n",
    "        if title_tag and date_tags:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = date_tags[-1].get_text(strip=True)\n",
    "            \n",
    "            # '24.10.31' 형식을 '2024.10.31'로 변환\n",
    "            date = f\"20{raw_date}\" if len(raw_date) == 8 else raw_date\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '중어중문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"중어중문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '영어영문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 영어영문학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://ysenglish.yonsei.ac.kr/ysenglish/process/process.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "        \n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '영어영문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"영어영문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '독어독문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 독어독문학과과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://ysgerman.yonsei.ac.kr/ysgerman/ger_notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '독어독문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"독어독문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '불어불문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 불어불문학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://franys.yonsei.ac.kr/franys/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '불어불문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"불어불문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '노어노문학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 노어노문학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://rus.yonsei.ac.kr/russia/borad_main.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            # 제목 텍스트를 불필요한 문자열을 제거하며 추출\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").replace(\"[학부]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '노어노문학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"노어노문학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '사학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 사학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://history.yonsei.ac.kr/histroy/02_announce/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            # 제목 텍스트를 불필요한 문자열을 제거하며 추출\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '사학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"사학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '철학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 철학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://philosophy.yonsei.ac.kr/cholhak/process/process.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '철학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"철학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '문헌정보학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 문헌정보학과\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://yslis.yonsei.ac.kr/doai/board/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '문헌정보학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"문헌정보학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '경제학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 경제학과\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import urllib3\n",
    "\n",
    "# SSL 경고 무시 설정\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# 기본 URL 설정\n",
    "base_url = \"https://econ.yonsei.ac.kr/econ/board/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"해당 페이지의 공지사항을 크롤링하는 함수\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 오프셋 설정\n",
    "    response = requests.get(base_url, params=params, verify=False)  # SSL 검증 비활성화\n",
    "    response.raise_for_status()  # 요청 성공 확인\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록에서 제목, 링크, 작성일 크롤링\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')  # 제목 추출\n",
    "        tds = row.select('td')  # 모든 <td> 요소 선택\n",
    "\n",
    "        # <td> 요소가 있는지 확인하고, 마지막 <td> 요소를 date로 추출\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])  # 절대 경로로 변환\n",
    "            raw_date = tds[-1].get_text(strip=True)  # 작성일 텍스트 추출\n",
    "\n",
    "            # 날짜가 \"YY.MM.DD\" 형식인 경우만 추출\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # \"20\"을 추가하여 \"YYYY.MM.DD\" 형식으로 변경\n",
    "            else:\n",
    "                continue  # 날짜 형식이 맞지 않으면 스킵\n",
    "\n",
    "            # 공지사항의 상세 페이지로 이동하여 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '경제학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 페이지에서 내용을 크롤링하는 함수\"\"\"\n",
    "    response = requests.get(link, verify=False)  # SSL 검증 비활성화\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용이 담긴 <div class=\"fr-view\">에서 텍스트 추출\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10  # 각 페이지 오프셋은 10씩 증가\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 후 제목과 작성일을 기준으로 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"경제학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '응용통계학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 응용통계학과\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://stat.yonsei.ac.kr/stat/board/under_notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# List to store all notices\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"Scrapes a page of notices based on the offset.\"\"\"\n",
    "    params['article.offset'] = offset  # Set page offset\n",
    "    response = requests.get(base_url, params=params, verify=False)  # Disable SSL verification\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all notices on the page\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')\n",
    "        tds = row.select('td')\n",
    "\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            raw_date = tds[-1].get_text(strip=True)\n",
    "\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # Convert to \"YYYY.MM.DD\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '응용통계학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"Scrapes the content of an individual notice.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Scraping the first 5 pages\n",
    "for page in range(5):\n",
    "    offset = page * 10\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# Remove duplicates based on title and date\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"응용통계학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '수학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 수학과\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://math.yonsei.ac.kr/math/math/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# List to store all notices\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"Scrapes a page of notices based on the offset.\"\"\"\n",
    "    params['article.offset'] = offset  # Set page offset\n",
    "    response = requests.get(base_url, params=params, verify=False)  # Disable SSL verification\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all notices on the page\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')\n",
    "        tds = row.select('td')\n",
    "\n",
    "        if title_tag and tds:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            raw_date = tds[-1].get_text(strip=True)\n",
    "\n",
    "            if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{2}$', raw_date):\n",
    "                date = f\"20{raw_date}\"  # Convert to \"YYYY.MM.DD\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '수학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"Scrapes the content of an individual notice.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Scraping the first 5 pages\n",
    "for page in range(5):\n",
    "    offset = page * 10\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# Remove duplicates based on title and date\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"수학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '물리학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 물리학과\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://physics.yonsei.ac.kr/notice/board\"\n",
    "params = {\n",
    "    'page': 1\n",
    "}\n",
    "\n",
    "# List to store all notices\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"Scrapes a page of notices based on the page number.\"\"\"\n",
    "    params['page'] = page  # Set page number\n",
    "    response = requests.get(base_url, params=params, verify=False)  # Disable SSL verification\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all rows containing notices\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('td.td-subject a')\n",
    "        date_tag = row.select_one('td.td-date')\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            date = date_tag.get_text(strip=True)\n",
    "\n",
    "            # Get context from the detailed page\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '물리학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"Scrapes the content of an individual notice.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract context from the notice content\n",
    "    context_tag = soup.select_one('div.bw_contents.editor_contents')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Scraping the first 5 pages\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# Remove duplicates based on title and date\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# Check if the dataframe is not empty\n",
    "if not df.empty:\n",
    "    # Save to CSV\n",
    "    csv_filename = \"물리학과_공지사항.csv\"\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n",
    "else:\n",
    "    print(\"데이터프레임이 비어 있습니다. HTML 구조를 확인해주세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '화학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 화학과\n",
    "\n",
    "# SSL 경고 비활성화\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://chemyonsei.kr/board/notice\"\n",
    "params = {\n",
    "    'p': 1\n",
    "}\n",
    "\n",
    "# 전체 공지사항 저장 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"해당 페이지의 공지사항 목록을 크롤링합니다.\"\"\"\n",
    "    params['p'] = page  # 페이지 번호 설정\n",
    "    response = requests.get(base_url, params=params, verify=False)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 공지사항 리스트에서 제목과 날짜를 찾습니다.\n",
    "    for row in soup.select('a.text-small'):\n",
    "        title_tag = row\n",
    "        date_tag = row.find_next('time')\n",
    "        \n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            date = date_tag.get_text(strip=True)\n",
    "\n",
    "            # 공지사항의 상세 내용을 크롤링합니다.\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '화학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"개별 공지사항의 상세 내용을 크롤링합니다.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용 크롤링\n",
    "    context_tag = soup.select_one('div.editor-contents')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 첫 5페이지 크롤링\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성 및 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"화학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 2학기 전자계산(2) 재개설 안내</td>\n",
       "      <td>2024.08.14</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100128?p=1</td>\n",
       "      <td>이과대학 전공기초 과목 중 2024학년도 1학기에 폐강되었던 '전자계산(2)'(학정...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>화학과</td>\n",
       "      <td>학점인정 담당 교수님</td>\n",
       "      <td>2024.02.08</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10075?p=1</td>\n",
       "      <td>▶ 물리분야 - 주상용 교수님▶ 분석분야 - 박성호 교수님▶ 유기분야 - 최수혁 교...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2025년도 미래장학금 제8기 장학생 모집 안내</td>\n",
       "      <td>2024.11.06</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100147?p=1</td>\n",
       "      <td>□ 1. 신청기간○ 2024. 11. 07(목) ~ 2024. 11. 14(목)□ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2025학년도 1학기 캠퍼스내 소속변경 전형 안내</td>\n",
       "      <td>2024.10.29</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100144?p=1</td>\n",
       "      <td>2025학년도 1학기 캠퍼스내 소속변경 전형 일정이 위와 같이 진행됩니다.자세한 내...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 2학기 학부 강의정보 공유 설문 실시 안내</td>\n",
       "      <td>2024.10.28</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100143?p=1</td>\n",
       "      <td>2. 2024학년도 2학기 학부 강의정보 공유 설문을 학사포탈시스템을 통하여 실시하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2025학년도 1학기 재입학 전형 안내</td>\n",
       "      <td>2024.10.28</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100142?p=1</td>\n",
       "      <td>2025학년도 1학기 재입학 전형 안내드립니다.-지원서류 접수기간: 11.25(월)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 2학기 졸업앨범 사진촬영 안내</td>\n",
       "      <td>2024.08.22</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100129?p=1</td>\n",
       "      <td>2024학년도 2학기 학부 졸업사진 촬영을 붙임 자료와 같이 진행할 예정이니 확인부...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 2학기 대학원 재입학 안내</td>\n",
       "      <td>2024.07.25</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100125?p=1</td>\n",
       "      <td>2024학년도 2학기 대학원 재입학을 다음과 같이 진행합니다.가. 지원자격1) 미등...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>화학과</td>\n",
       "      <td>[대학원]2024학년도 2학기 대학원 휴학 및 복학 신청 및 학적 관련 안내</td>\n",
       "      <td>2024.07.15</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100120?p=1</td>\n",
       "      <td>대학원의 2024학년도 2학기 대학원 휴학 및 복학 신청 및 학적 관련 안내드립니다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 2학기 캠퍼스내/간 재학생 복수전공 신청 안내</td>\n",
       "      <td>2024.06.13</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100110?p=1</td>\n",
       "      <td>1. 2024학년도 2학기 캠퍼스내/간 재학생복수전공(구.이중 및 다중전공) 신청을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 2학기 캠퍼스내/졸업예정자 복수전공, 연계전공 신청 안내</td>\n",
       "      <td>2024.06.13</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100109?p=1</td>\n",
       "      <td>1. 2024학년도 2학기 캠퍼스내, 졸업예정자 복수전공 및 연계전공 및 신청 안내...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 2학기 학위과정 변경 신청 안내</td>\n",
       "      <td>2024.06.13</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100108?p=2</td>\n",
       "      <td>대학원의 2024학년도 2학기 학위과정 변경 신청 안내드립니다.1. 2024학년도 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024-2학기 AI융합심화전공 대상자 선발 진행 안내</td>\n",
       "      <td>2024.06.04</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100105?p=2</td>\n",
       "      <td>2024-2학기 AI융합심화전공 대상자 선발 진행 안내드립니다.가. 선발 일정1) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 2학기 재입학 전형 안내</td>\n",
       "      <td>2024.05.13</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/100102?p=2</td>\n",
       "      <td>2024학년도 2학기 재입학 전형 안내드립니다.-지원서류 접수기간: 5. 29.(수...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 1학기 S/U평가 신청 및 S/U평가 가능 교과목 안내</td>\n",
       "      <td>2024.04.25</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10097?p=2</td>\n",
       "      <td>1. 관련: 학사지원팀-1376 ~ 1391 (2024. 3. 27.), 「2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024-1학기 학부 수강과목 철회 안내</td>\n",
       "      <td>2024.04.22</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10094?p=2</td>\n",
       "      <td>2024학년도 1학기 학부 수강과목 철회를 다음과 같이 실시하오니 참고 부탁드립니다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024년도 전기 자연계대학원 전문연구요원 편입대상자 선발 공고 안내</td>\n",
       "      <td>2024.04.22</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10093?p=2</td>\n",
       "      <td>1. 관련: 학술진흥팀-10409 (2024. 4. 16.) 「2024년도 전기 자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>화학과</td>\n",
       "      <td>교직과정 이수예정자 선발 전형 안내</td>\n",
       "      <td>2024.04.15</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10090?p=2</td>\n",
       "      <td>2025학년도 교직과정 이수예정자 선발 전형 안내드립니다.교원양성실무위원회에서는 교...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 1학기 졸업앨범 사진촬영 안내</td>\n",
       "      <td>2024.04.15</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10089?p=2</td>\n",
       "      <td>2024학년도 1학기 학부 졸업사진 촬영을 붙임과 같이 진행할 예정이니 확인 부탁드...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024년도 자연과학연구원전문연구요원(병역특례) 편입신청 안내</td>\n",
       "      <td>2024.02.21</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10077?p=2</td>\n",
       "      <td>2024년도 자연과학연구원전문연구요원을 선발하오니 전문연구요원 편입신청 지원자는 접...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 1학기 대학원 재입학 안내</td>\n",
       "      <td>2024.01.18</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10074?p=3</td>\n",
       "      <td>2024학년도 1학기 대학원 재입학 안내드립니다.-지원자격: 미등록, 휴학만료, 자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 1학기 전자계산(2) 폐강 안내</td>\n",
       "      <td>2024.01.08</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10073?p=3</td>\n",
       "      <td>2024학년도 이과대학 전공기초 과목인 '전자계산(2)' (학정번호: SCI2002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024-1학기 AI융합심화전공 대상자 선발 진행 안내</td>\n",
       "      <td>2024.01.04</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10071?p=3</td>\n",
       "      <td>2024-1학기 AI융합심화전공 대상자 선발 진행 안내드립니다.가. 선발 일정1) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 1학기 대학원 휴학 및 복학 기간 안내</td>\n",
       "      <td>2023.12.26</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10070?p=3</td>\n",
       "      <td>2024-1학기 대학원 휴학 및 복학신청 안내드립니다.첨부파일 참고 부탁드립니다.가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 1학기 석·박사 통합 과정 중단 신청 안내</td>\n",
       "      <td>2023.12.19</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10069?p=3</td>\n",
       "      <td>2024학년도 1학기 석·박사 통합 과정 중단 신청 안내드립니다.-학생 신청 기간:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 1학기 석사과정에서 석·박사통합과정으로의 학위과정 변경 신청 안내</td>\n",
       "      <td>2023.12.19</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10068?p=3</td>\n",
       "      <td>2024학년도 1학기 석사과정에서 석·박사통합과정으로의 학위과정 변경 신청 안내드립...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 1학기 캠퍼스내/졸업예정자 복수전공,연계전공 신청 안내</td>\n",
       "      <td>2023.12.11</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10067?p=3</td>\n",
       "      <td>2024학년도 1학기 캠퍼스내/졸업예정자 복수전공, 연계전공 신청 안내드립니다.자세...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 1학기 재입학 전형 안내</td>\n",
       "      <td>2023.12.04</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10065?p=3</td>\n",
       "      <td>2024학년도 1학기 재입학 전형 안내드립니다.자세한 사항은 첨부파일 참고바랍니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2024학년도 1학기 캠퍼스내 소속변경 전형 안내</td>\n",
       "      <td>2023.11.01</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10039?p=3</td>\n",
       "      <td>2024학년도 1학기 캠퍼스내 소속변경 전형 일정이 위와 같이 진행됩니다.자세한 내...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2023학년도 2학기 재입학 전형 안내</td>\n",
       "      <td>2023.10.04</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10033?p=3</td>\n",
       "      <td>자세한 내용은 첨부파일 참조바랍니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2023학년도 2학기 캠퍼스내/졸업예정자 복수전공, 연계전공 신청 안내</td>\n",
       "      <td>2023.10.04</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10032?p=4</td>\n",
       "      <td>전형별 주요일정은 위와 같습니다.자세한 내용은 첨부파일 참조 바랍니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2023 Nature Conference</td>\n",
       "      <td>2023.10.04</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10021?p=4</td>\n",
       "      <td>공학연구원에서 교내 연구력 향상 및 해외 우수 연구자와의 공동 연구협력 촉진을 위하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>화학과</td>\n",
       "      <td>[자연과학연구원] 2023년도 전문연구요원(병역특례) 편입신청 안내(2023.8.2...</td>\n",
       "      <td>2023.10.04</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10020?p=4</td>\n",
       "      <td>2023년도 병역지정업체(연구기관)별 전문연구요원 인원배정 결과에 따라 전문연구요원...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2023년도 전문연구요원(병역특례) 편입신청 지원자 수요 인원 조사</td>\n",
       "      <td>2023.10.04</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10019?p=4</td>\n",
       "      <td>안녕하세요. 화학과 사무실입니다.2023년도 병역지정업체(연구기관)별 전문연구요원을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>화학과</td>\n",
       "      <td>2023-2학기 AI융합심화전공 프로그램 안내</td>\n",
       "      <td>2023.10.04</td>\n",
       "      <td>https://chemyonsei.kr/board/notice/10018?p=4</td>\n",
       "      <td>안녕하세요. 화학과의 경우 2023-2학기부터 AI화학융합심화전공이 개설됩니다.신청...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   department                                              title        date  \\\n",
       "0         화학과                         2024학년도 2학기 전자계산(2) 재개설 안내  2024.08.14   \n",
       "1         화학과                                        학점인정 담당 교수님  2024.02.08   \n",
       "2         화학과                         2025년도 미래장학금 제8기 장학생 모집 안내  2024.11.06   \n",
       "3         화학과                        2025학년도 1학기 캠퍼스내 소속변경 전형 안내  2024.10.29   \n",
       "4         화학과                    2024학년도 2학기 학부 강의정보 공유 설문 실시 안내  2024.10.28   \n",
       "5         화학과                              2025학년도 1학기 재입학 전형 안내  2024.10.28   \n",
       "6         화학과                           2024학년도 2학기 졸업앨범 사진촬영 안내  2024.08.22   \n",
       "8         화학과                             2024학년도 2학기 대학원 재입학 안내  2024.07.25   \n",
       "9         화학과         [대학원]2024학년도 2학기 대학원 휴학 및 복학 신청 및 학적 관련 안내  2024.07.15   \n",
       "10        화학과                  2024학년도 2학기 캠퍼스내/간 재학생 복수전공 신청 안내  2024.06.13   \n",
       "11        화학과            2024학년도 2학기 캠퍼스내/졸업예정자 복수전공, 연계전공 신청 안내  2024.06.13   \n",
       "14        화학과                          2024학년도 2학기 학위과정 변경 신청 안내  2024.06.13   \n",
       "15        화학과                     2024-2학기 AI융합심화전공 대상자 선발 진행 안내  2024.06.04   \n",
       "16        화학과                              2024학년도 2학기 재입학 전형 안내  2024.05.13   \n",
       "17        화학과             2024학년도 1학기 S/U평가 신청 및 S/U평가 가능 교과목 안내  2024.04.25   \n",
       "18        화학과                             2024-1학기 학부 수강과목 철회 안내  2024.04.22   \n",
       "19        화학과             2024년도 전기 자연계대학원 전문연구요원 편입대상자 선발 공고 안내  2024.04.22   \n",
       "20        화학과                                교직과정 이수예정자 선발 전형 안내  2024.04.15   \n",
       "21        화학과                           2024학년도 1학기 졸업앨범 사진촬영 안내  2024.04.15   \n",
       "22        화학과                 2024년도 자연과학연구원전문연구요원(병역특례) 편입신청 안내  2024.02.21   \n",
       "26        화학과                             2024학년도 1학기 대학원 재입학 안내  2024.01.18   \n",
       "27        화학과                          2024학년도 1학기 전자계산(2) 폐강 안내  2024.01.08   \n",
       "28        화학과                     2024-1학기 AI융합심화전공 대상자 선발 진행 안내  2024.01.04   \n",
       "29        화학과                      2024학년도 1학기 대학원 휴학 및 복학 기간 안내  2023.12.26   \n",
       "30        화학과                    2024학년도 1학기 석·박사 통합 과정 중단 신청 안내  2023.12.19   \n",
       "31        화학과       2024학년도 1학기 석사과정에서 석·박사통합과정으로의 학위과정 변경 신청 안내  2023.12.19   \n",
       "32        화학과             2024학년도 1학기 캠퍼스내/졸업예정자 복수전공,연계전공 신청 안내  2023.12.11   \n",
       "33        화학과                              2024학년도 1학기 재입학 전형 안내  2023.12.04   \n",
       "34        화학과                        2024학년도 1학기 캠퍼스내 소속변경 전형 안내  2023.11.01   \n",
       "35        화학과                              2023학년도 2학기 재입학 전형 안내  2023.10.04   \n",
       "38        화학과            2023학년도 2학기 캠퍼스내/졸업예정자 복수전공, 연계전공 신청 안내  2023.10.04   \n",
       "39        화학과                             2023 Nature Conference  2023.10.04   \n",
       "40        화학과  [자연과학연구원] 2023년도 전문연구요원(병역특례) 편입신청 안내(2023.8.2...  2023.10.04   \n",
       "41        화학과              2023년도 전문연구요원(병역특례) 편입신청 지원자 수요 인원 조사  2023.10.04   \n",
       "42        화학과                          2023-2학기 AI융합심화전공 프로그램 안내  2023.10.04   \n",
       "\n",
       "                                             link  \\\n",
       "0   https://chemyonsei.kr/board/notice/100128?p=1   \n",
       "1    https://chemyonsei.kr/board/notice/10075?p=1   \n",
       "2   https://chemyonsei.kr/board/notice/100147?p=1   \n",
       "3   https://chemyonsei.kr/board/notice/100144?p=1   \n",
       "4   https://chemyonsei.kr/board/notice/100143?p=1   \n",
       "5   https://chemyonsei.kr/board/notice/100142?p=1   \n",
       "6   https://chemyonsei.kr/board/notice/100129?p=1   \n",
       "8   https://chemyonsei.kr/board/notice/100125?p=1   \n",
       "9   https://chemyonsei.kr/board/notice/100120?p=1   \n",
       "10  https://chemyonsei.kr/board/notice/100110?p=1   \n",
       "11  https://chemyonsei.kr/board/notice/100109?p=1   \n",
       "14  https://chemyonsei.kr/board/notice/100108?p=2   \n",
       "15  https://chemyonsei.kr/board/notice/100105?p=2   \n",
       "16  https://chemyonsei.kr/board/notice/100102?p=2   \n",
       "17   https://chemyonsei.kr/board/notice/10097?p=2   \n",
       "18   https://chemyonsei.kr/board/notice/10094?p=2   \n",
       "19   https://chemyonsei.kr/board/notice/10093?p=2   \n",
       "20   https://chemyonsei.kr/board/notice/10090?p=2   \n",
       "21   https://chemyonsei.kr/board/notice/10089?p=2   \n",
       "22   https://chemyonsei.kr/board/notice/10077?p=2   \n",
       "26   https://chemyonsei.kr/board/notice/10074?p=3   \n",
       "27   https://chemyonsei.kr/board/notice/10073?p=3   \n",
       "28   https://chemyonsei.kr/board/notice/10071?p=3   \n",
       "29   https://chemyonsei.kr/board/notice/10070?p=3   \n",
       "30   https://chemyonsei.kr/board/notice/10069?p=3   \n",
       "31   https://chemyonsei.kr/board/notice/10068?p=3   \n",
       "32   https://chemyonsei.kr/board/notice/10067?p=3   \n",
       "33   https://chemyonsei.kr/board/notice/10065?p=3   \n",
       "34   https://chemyonsei.kr/board/notice/10039?p=3   \n",
       "35   https://chemyonsei.kr/board/notice/10033?p=3   \n",
       "38   https://chemyonsei.kr/board/notice/10032?p=4   \n",
       "39   https://chemyonsei.kr/board/notice/10021?p=4   \n",
       "40   https://chemyonsei.kr/board/notice/10020?p=4   \n",
       "41   https://chemyonsei.kr/board/notice/10019?p=4   \n",
       "42   https://chemyonsei.kr/board/notice/10018?p=4   \n",
       "\n",
       "                                              context  \n",
       "0   이과대학 전공기초 과목 중 2024학년도 1학기에 폐강되었던 '전자계산(2)'(학정...  \n",
       "1   ▶ 물리분야 - 주상용 교수님▶ 분석분야 - 박성호 교수님▶ 유기분야 - 최수혁 교...  \n",
       "2   □ 1. 신청기간○ 2024. 11. 07(목) ~ 2024. 11. 14(목)□ ...  \n",
       "3   2025학년도 1학기 캠퍼스내 소속변경 전형 일정이 위와 같이 진행됩니다.자세한 내...  \n",
       "4   2. 2024학년도 2학기 학부 강의정보 공유 설문을 학사포탈시스템을 통하여 실시하...  \n",
       "5   2025학년도 1학기 재입학 전형 안내드립니다.-지원서류 접수기간: 11.25(월)...  \n",
       "6   2024학년도 2학기 학부 졸업사진 촬영을 붙임 자료와 같이 진행할 예정이니 확인부...  \n",
       "8   2024학년도 2학기 대학원 재입학을 다음과 같이 진행합니다.가. 지원자격1) 미등...  \n",
       "9   대학원의 2024학년도 2학기 대학원 휴학 및 복학 신청 및 학적 관련 안내드립니다...  \n",
       "10  1. 2024학년도 2학기 캠퍼스내/간 재학생복수전공(구.이중 및 다중전공) 신청을...  \n",
       "11  1. 2024학년도 2학기 캠퍼스내, 졸업예정자 복수전공 및 연계전공 및 신청 안내...  \n",
       "14  대학원의 2024학년도 2학기 학위과정 변경 신청 안내드립니다.1. 2024학년도 ...  \n",
       "15  2024-2학기 AI융합심화전공 대상자 선발 진행 안내드립니다.가. 선발 일정1) ...  \n",
       "16  2024학년도 2학기 재입학 전형 안내드립니다.-지원서류 접수기간: 5. 29.(수...  \n",
       "17  1. 관련: 학사지원팀-1376 ~ 1391 (2024. 3. 27.), 「2024...  \n",
       "18  2024학년도 1학기 학부 수강과목 철회를 다음과 같이 실시하오니 참고 부탁드립니다...  \n",
       "19  1. 관련: 학술진흥팀-10409 (2024. 4. 16.) 「2024년도 전기 자...  \n",
       "20  2025학년도 교직과정 이수예정자 선발 전형 안내드립니다.교원양성실무위원회에서는 교...  \n",
       "21  2024학년도 1학기 학부 졸업사진 촬영을 붙임과 같이 진행할 예정이니 확인 부탁드...  \n",
       "22  2024년도 자연과학연구원전문연구요원을 선발하오니 전문연구요원 편입신청 지원자는 접...  \n",
       "26  2024학년도 1학기 대학원 재입학 안내드립니다.-지원자격: 미등록, 휴학만료, 자...  \n",
       "27  2024학년도 이과대학 전공기초 과목인 '전자계산(2)' (학정번호: SCI2002...  \n",
       "28  2024-1학기 AI융합심화전공 대상자 선발 진행 안내드립니다.가. 선발 일정1) ...  \n",
       "29  2024-1학기 대학원 휴학 및 복학신청 안내드립니다.첨부파일 참고 부탁드립니다.가...  \n",
       "30  2024학년도 1학기 석·박사 통합 과정 중단 신청 안내드립니다.-학생 신청 기간:...  \n",
       "31  2024학년도 1학기 석사과정에서 석·박사통합과정으로의 학위과정 변경 신청 안내드립...  \n",
       "32  2024학년도 1학기 캠퍼스내/졸업예정자 복수전공, 연계전공 신청 안내드립니다.자세...  \n",
       "33  2024학년도 1학기 재입학 전형 안내드립니다.자세한 사항은 첨부파일 참고바랍니다....  \n",
       "34  2024학년도 1학기 캠퍼스내 소속변경 전형 일정이 위와 같이 진행됩니다.자세한 내...  \n",
       "35                               자세한 내용은 첨부파일 참조바랍니다.  \n",
       "38            전형별 주요일정은 위와 같습니다.자세한 내용은 첨부파일 참조 바랍니다.  \n",
       "39  공학연구원에서 교내 연구력 향상 및 해외 우수 연구자와의 공동 연구협력 촉진을 위하...  \n",
       "40  2023년도 병역지정업체(연구기관)별 전문연구요원 인원배정 결과에 따라 전문연구요원...  \n",
       "41  안녕하세요. 화학과 사무실입니다.2023년도 병역지정업체(연구기관)별 전문연구요원을...  \n",
       "42  안녕하세요. 화학과의 경우 2023-2학기부터 AI화학융합심화전공이 개설됩니다.신청...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "CSV 파일이 '지구시스템과학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 지구시스템과학과\n",
    "\n",
    "# SSL 경고 비활성화\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"http://geo.yonsei.ac.kr/index.php\"\n",
    "params = {\n",
    "    'hCode': 'BOARD',\n",
    "    'bo_idx': 2,\n",
    "    'page': 'list',\n",
    "    'pg': 1\n",
    "}\n",
    "\n",
    "# 전체 공지사항 저장 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"해당 페이지의 공지사항 목록을 크롤링합니다.\"\"\"\n",
    "    params['pg'] = page  # 페이지 번호 설정\n",
    "    response = requests.get(base_url, params=params, verify=False)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 공지사항 리스트에서 제목과 날짜를 찾습니다.\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a[href^=\"?page=view\"]')\n",
    "        date_tag = row.select_one('td:nth-last-child(1)')  # 날짜가 마지막 열에 있다고 가정\n",
    "        \n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            date = date_tag.get_text(strip=True)\n",
    "\n",
    "            # 공지사항의 상세 내용을 크롤링합니다.\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '지구시스템과학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"개별 공지사항의 상세 내용을 크롤링합니다.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용 크롤링\n",
    "    context_tag = soup.select_one('div.board_content')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# 첫 5페이지 크롤링\n",
    "for page in range(1, 3):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성 및 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"지구시스템과학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '천문우주학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 천문우주학과\n",
    "\n",
    "# SSL 경고 비활성화\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://astro.yonsei.ac.kr/galaxy/galaxy01/notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'article.offset': 0,\n",
    "    'articleLimit': 10\n",
    "}\n",
    "\n",
    "# 전체 공지사항 저장 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(offset):\n",
    "    \"\"\"페이지의 공지사항 목록을 크롤링합니다.\"\"\"\n",
    "    params['article.offset'] = offset  # 페이지 offset 설정\n",
    "    response = requests.get(base_url, params=params, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 리스트에서 제목과 날짜를 찾습니다.\n",
    "    for row in soup.select('tr'):\n",
    "        title_tag = row.select_one('a.c-board-title')\n",
    "        date_tag = row.select_one('td:nth-last-child(1)')  # 날짜가 마지막 열에 위치\n",
    "        \n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = urljoin(base_url, title_tag['href'])\n",
    "            raw_date = date_tag.get_text(strip=True)\n",
    "\n",
    "            # 날짜 형식을 변환\n",
    "            if raw_date and len(raw_date) == 8:  # e.g., '24.10.30'\n",
    "                year, month, day = raw_date.split('.')\n",
    "                date = f\"20{year}.{month}.{day}\"  # YYYY.MM.DD 형식으로 변환\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # 공지사항의 상세 내용을 크롤링합니다.\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '천문우주학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"개별 공지사항의 상세 내용을 크롤링합니다.\"\"\"\n",
    "    response = requests.get(link, verify=False)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용 크롤링\n",
    "    context_tag = soup.select_one('div.fr-view')\n",
    "    context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "    return context\n",
    "\n",
    "# 첫 5페이지 크롤링\n",
    "for page in range(5):\n",
    "    offset = page * 10\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(offset)\n",
    "\n",
    "# 데이터프레임 생성 및 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"천문우주학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대기과학과 공지사항 크롤링 중...\n",
      "CSV 파일이 '대기과학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 대기과학과 와 이건 왜 다르게 생겼냐 ㅠㅡㅠㅠㅡㅠㅡㅠㅡㅠ\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://atmos.yonsei.ac.kr/categories/%EA%B3%B5%EC%A7%80%EC%82%AC%ED%95%AD/\"\n",
    "all_notices = []\n",
    "\n",
    "def scrape_all_notices():\n",
    "    \"\"\"대기과학과 공지사항 전체 크롤링\"\"\"\n",
    "    response = requests.get(base_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 아이템을 묶어서 크롤링\n",
    "    rows = soup.select('div.col-xs-11')  # 제목과 날짜를 포함하는 부모 태그 선택\n",
    "\n",
    "    for row in rows:\n",
    "        # 제목과 링크 추출\n",
    "        title_tag = row.select_one('h3 a')\n",
    "        date_tag = row.select_one('p.date-comments i.fa.fa-calendar-o')\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)  # 제목 추출\n",
    "            link = title_tag['href']  # 링크 추출\n",
    "            raw_date = date_tag.next_sibling.strip()  # 날짜 텍스트 추출\n",
    "            date = convert_date(raw_date)\n",
    "\n",
    "            # 상세 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            all_notices.append({\n",
    "                'department': '대기과학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def convert_date(raw_date):\n",
    "    \"\"\"날짜를 'YYYY.MM.DD' 형식으로 변환\"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(raw_date, '%B %d, %Y').strftime('%Y.%m.%d')\n",
    "    except ValueError:\n",
    "        return raw_date\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"개별 공지사항의 상세 내용을 크롤링\"\"\"\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 404:\n",
    "        return \"링크를 찾을 수 없습니다.\"\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용 크롤링\n",
    "    content_tag = soup.select_one('div#post-content')\n",
    "    context = content_tag.get_text(strip=True) if content_tag else \"내용 없음\"\n",
    "\n",
    "    return context\n",
    "\n",
    "# 공지사항 크롤링 실행\n",
    "print(\"대기과학과 공지사항 크롤링 중...\")\n",
    "scrape_all_notices()\n",
    "\n",
    "# 데이터프레임 생성 및 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"대기과학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '화공생명공학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 화공생명공학과\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 공지사항을 저장할 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"화공생명공학과 공지사항 특정 페이지 크롤링\"\"\"\n",
    "    base_url = f\"https://chemeng.yonsei.ac.kr/?c=209&s=209&gbn=list&gp={page}\"\n",
    "    response = requests.get(base_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 목록 테이블의 모든 행 선택\n",
    "    rows = soup.select('#bbsStandardWrap > table > tbody > tr')\n",
    "\n",
    "    for row in rows:\n",
    "        # 제목과 링크 추출\n",
    "        title_tag = row.select_one('td.col-tit > a')\n",
    "        if title_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = \"https://chemeng.yonsei.ac.kr\" + title_tag['href']  # 상대 경로를 절대 경로로 변환\n",
    "        else:\n",
    "            title = \"제목 없음\"\n",
    "            link = None\n",
    "\n",
    "        # 날짜 추출\n",
    "        date_tag = row.select_one('td.col-date')\n",
    "        date = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "\n",
    "        # 중복 확인: 동일한 title과 date가 이미 추가된 경우 스킵\n",
    "        if any(notice for notice in all_notices if notice['title'] == title and notice['date'] == date):\n",
    "            continue\n",
    "\n",
    "        # 상세 내용 크롤링\n",
    "        context = scrape_notice_context(link) if link else \"내용 없음\"\n",
    "\n",
    "        # 공지사항 추가\n",
    "        all_notices.append({\n",
    "            'department': '화공생명공학과',\n",
    "            'title': title,\n",
    "            'date': date,\n",
    "            'link': link,\n",
    "            'context': context\n",
    "        })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"개별 공지사항의 상세 내용을 크롤링\"\"\"\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 상세 내용 크롤링\n",
    "    content_tag = soup.select_one('#bbsContents')\n",
    "    context = content_tag.get_text(strip=True) if content_tag else \"내용 없음\"\n",
    "    return context\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(all_notices)\n",
    "\n",
    "# 중복 제거 (title과 date 기준)\n",
    "df = df.drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"화공생명공학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공지사항 크롤링 시작...\n",
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "크롤링 완료. '전기전자공학과_공지사항.csv' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 전기전자공학과\n",
    "\n",
    "# 공지사항 URL 설정\n",
    "base_url = \"https://ee.yonsei.ac.kr/ee/community/academic_notice.do\"\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'articleLimit': 10,\n",
    "    'article.offset': 0  # 페이지 오프셋 초기화\n",
    "}\n",
    "\n",
    "# 데이터 저장 리스트\n",
    "all_notices = []\n",
    "\n",
    "# 날짜 변환 함수\n",
    "def format_date(raw_date):\n",
    "    match = re.match(r'(\\d{2})\\.(\\d{2})\\.(\\d{2})', raw_date)\n",
    "    if match:\n",
    "        return f\"20{match.group(1)}.{match.group(2)}.{match.group(3)}\"\n",
    "    return \"날짜 없음\"\n",
    "\n",
    "# 공지사항 크롤링 함수\n",
    "def scrape_page(offset):\n",
    "    \"\"\"페이지 단위로 공지사항 크롤링\"\"\"\n",
    "    params['article.offset'] = offset\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 공지사항 추출\n",
    "    for row in soup.select('tr'):\n",
    "        # 제목과 링크 추출\n",
    "        title_tag = row.select_one('a.c-board-title')\n",
    "        if not title_tag:\n",
    "            continue\n",
    "        title = title_tag.get_text(strip=True).replace(\"[공지]\", \"\").strip()\n",
    "        link = urljoin(base_url, title_tag['href'])\n",
    "\n",
    "        # 날짜 추출\n",
    "        date_tag = row.select_one('td:nth-child(5)')\n",
    "        raw_date = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "        formatted_date = format_date(raw_date)\n",
    "\n",
    "        # 상세 내용 추출\n",
    "        context = scrape_notice_context(link)\n",
    "\n",
    "        # 데이터 저장\n",
    "        all_notices.append({\n",
    "            'department': '전기전자공학과',\n",
    "            'title': title,\n",
    "            'date': formatted_date,\n",
    "            'link': link,\n",
    "            'context': context\n",
    "        })\n",
    "\n",
    "# 상세 페이지 내용 크롤링 함수\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"공지사항 상세 내용 크롤링\"\"\"\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 공지사항 내용 추출\n",
    "        context_tag = soup.select_one('div.fr-view')\n",
    "        context = context_tag.get_text(strip=True) if context_tag else \"내용 없음\"\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching context for link {link}: {e}\")\n",
    "        return \"링크 오류\"\n",
    "\n",
    "# 전체 페이지 크롤링 실행\n",
    "print(\"공지사항 크롤링 시작...\")\n",
    "for page in range(5):  # 5페이지 크롤링\n",
    "    print(f\"{page + 1}페이지 크롤링 중...\")\n",
    "    scrape_page(page * 10)\n",
    "\n",
    "# 데이터프레임 생성 및 중복 제거\n",
    "df = pd.DataFrame(all_notices).drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"전기전자공학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "print(f\"크롤링 완료. '{csv_filename}' 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 크롤링 중...\n",
      "2페이지 크롤링 중...\n",
      "3페이지 크롤링 중...\n",
      "4페이지 크롤링 중...\n",
      "5페이지 크롤링 중...\n",
      "CSV 파일이 '건축공학과_공지사항.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 공지사항 저장 리스트\n",
    "all_notices = []\n",
    "\n",
    "def scrape_page(page):\n",
    "    \"\"\"건축공학과 공지사항 특정 페이지 크롤링\"\"\"\n",
    "    url = f\"https://arch.yonsei.ac.kr/notice/page/{page}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"페이지 {page}에 접속할 수 없습니다. 상태 코드: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    rows = soup.select('#body > div.dcore.dcore-list.dcore-notice > div.overflow-x-outer > div > table > tbody > tr')\n",
    "\n",
    "    for row in rows:\n",
    "        # 제목과 링크 추출\n",
    "        title_tag = row.select_one('td.title > a')\n",
    "        date_tag = row.select_one('td.packed.hide-on-small-only')\n",
    "\n",
    "        if title_tag and date_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = \"https://arch.yonsei.ac.kr\" + title_tag['href']\n",
    "            raw_date = date_tag.get_text(strip=True)\n",
    "\n",
    "            # 날짜 형식 변환\n",
    "            date = re.sub(r'(\\d{4})\\.(\\d{2})\\.(\\d{2})', r'\\1.\\2.\\3', raw_date)\n",
    "\n",
    "            # 중복 제거\n",
    "            if any(notice for notice in all_notices if notice['title'] == title and notice['date'] == date):\n",
    "                continue\n",
    "\n",
    "            # 상세 내용 크롤링\n",
    "            context = scrape_notice_context(link)\n",
    "\n",
    "            # 공지사항 추가\n",
    "            all_notices.append({\n",
    "                'department': '건축공학과',\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'link': link,\n",
    "                'context': context\n",
    "            })\n",
    "\n",
    "def scrape_notice_context(link):\n",
    "    \"\"\"개별 공지사항의 상세 내용을 크롤링\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(link, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return \"내용 없음\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_tag = soup.select_one('#body > div.dcore.dcore-view.dcore-notice')\n",
    "    return content_tag.get_text(strip=True) if content_tag else \"내용 없음\"\n",
    "\n",
    "# 1페이지부터 5페이지까지 크롤링\n",
    "for page in range(1, 6):\n",
    "    print(f\"{page}페이지 크롤링 중...\")\n",
    "    scrape_page(page)\n",
    "    time.sleep(2)  # 요청 간 딜레이 추가\n",
    "\n",
    "# 데이터프레임 생성 및 중복 제거\n",
    "df = pd.DataFrame(all_notices)\n",
    "df = df.drop_duplicates(subset=['title', 'date'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_filename = \"건축공학과_공지사항.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{csv_filename}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>건축공학과</td>\n",
       "      <td>2025년 2월 건축공학(4년제) 졸업대상자 Exit Interview 공지2024...</td>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>https://arch.yonsei.ac.kr/notice/2025%eb%85%84...</td>\n",
       "      <td>2025년 2월 건축공학(4년제) 졸업대상자 Exit Interview 공지졸업 예...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>건축공학과</td>\n",
       "      <td>IPESK 차세대 공학자 추천 신청 안내 (대학원생 대상)2024-11-20</td>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>https://arch.yonsei.ac.kr/notice/ipesk-%ec%b0%...</td>\n",
       "      <td>IPESK 차세대 공학자 추천 신청 안내 (대학원생 대상)사단법인 한국이공학진흥원(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>건축공학과</td>\n",
       "      <td>2024-2 건축공학과 대학원 원우회 정보 공유2024-11-19</td>\n",
       "      <td>2024-11-19</td>\n",
       "      <td>https://arch.yonsei.ac.kr/notice/2024-2-%ea%b1...</td>\n",
       "      <td>2024-2 건축공학과 대학원 원우회 정보 공유2024년 11월 15일에 진행된 건...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>건축공학과</td>\n",
       "      <td>2024-2학기 논문 본심사 상세일정 및 제출서류 안내2024-11-12</td>\n",
       "      <td>2024-11-12</td>\n",
       "      <td>https://arch.yonsei.ac.kr/notice/2024-2%ed%95%...</td>\n",
       "      <td>2024-2학기 논문 본심사 상세일정 및 제출서류 안내2024-2학기 논문 본심사를...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>건축공학과</td>\n",
       "      <td>2024-2학기 석박사 자격시험 신청 안내2024-10-29</td>\n",
       "      <td>2024-10-29</td>\n",
       "      <td>https://arch.yonsei.ac.kr/notice/2024-2%ed%95%...</td>\n",
       "      <td>2024-2학기 석박사 자격시험 신청 안내2024-2학기 석박사 자격시험을 아래와 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>건축공학과</td>\n",
       "      <td>2023-2학기 학위논문 제출 안내2023-12-26</td>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>https://arch.yonsei.ac.kr/notice/2023-2%ed%95%...</td>\n",
       "      <td>2023-2학기 학위논문 제출 안내2023학년도 2학기 학위논문 제출시스템이 개편되...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>건축공학과</td>\n",
       "      <td>2024-1 통합과정 중단 신청 안내2023-12-19</td>\n",
       "      <td>2023-12-19</td>\n",
       "      <td>https://arch.yonsei.ac.kr/notice/2024-1-%ed%86...</td>\n",
       "      <td>2024-1 통합과정 중단 신청 안내2024학년도 1학기 통합과정 중단 신청을 다음...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>건축공학과</td>\n",
       "      <td>2024-1학기 석사에서 통합으로의 학위과정 변경 신청 안내2023-12-19</td>\n",
       "      <td>2023-12-19</td>\n",
       "      <td>https://arch.yonsei.ac.kr/notice/2024-1%ed%95%...</td>\n",
       "      <td>2024-1학기 석사에서 통합으로의 학위과정 변경 신청 안내2024학년도 1학기 석...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>건축공학과</td>\n",
       "      <td>2024학년도 연세대학교 건축공학과 강사 채용 (23.12.15~12.20)2023...</td>\n",
       "      <td>2023-12-15</td>\n",
       "      <td>https://arch.yonsei.ac.kr/notice/2024%ed%95%99...</td>\n",
       "      <td>2024학년도 연세대학교 건축공학과 강사 채용 (23.12.15~12.20)2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>건축공학과</td>\n",
       "      <td>2023-2 학위논문 본심사 상세일정 - 12/18(월) (일정 업데이트)2023-...</td>\n",
       "      <td>2023-12-11</td>\n",
       "      <td>https://arch.yonsei.ac.kr/notice/2023-2-%ed%95...</td>\n",
       "      <td>2023-2 학위논문 본심사 상세일정 - 12/18(월) (일정 업데이트)2023-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   department                                              title        date  \\\n",
       "0       건축공학과  2025년 2월 건축공학(4년제) 졸업대상자 Exit Interview 공지2024...  2024-11-20   \n",
       "1       건축공학과         IPESK 차세대 공학자 추천 신청 안내 (대학원생 대상)2024-11-20  2024-11-20   \n",
       "2       건축공학과               2024-2 건축공학과 대학원 원우회 정보 공유2024-11-19  2024-11-19   \n",
       "3       건축공학과           2024-2학기 논문 본심사 상세일정 및 제출서류 안내2024-11-12  2024-11-12   \n",
       "4       건축공학과                  2024-2학기 석박사 자격시험 신청 안내2024-10-29  2024-10-29   \n",
       "..        ...                                                ...         ...   \n",
       "75      건축공학과                      2023-2학기 학위논문 제출 안내2023-12-26  2023-12-26   \n",
       "76      건축공학과                     2024-1 통합과정 중단 신청 안내2023-12-19  2023-12-19   \n",
       "77      건축공학과        2024-1학기 석사에서 통합으로의 학위과정 변경 신청 안내2023-12-19  2023-12-19   \n",
       "78      건축공학과  2024학년도 연세대학교 건축공학과 강사 채용 (23.12.15~12.20)2023...  2023-12-15   \n",
       "79      건축공학과  2023-2 학위논문 본심사 상세일정 - 12/18(월) (일정 업데이트)2023-...  2023-12-11   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://arch.yonsei.ac.kr/notice/2025%eb%85%84...   \n",
       "1   https://arch.yonsei.ac.kr/notice/ipesk-%ec%b0%...   \n",
       "2   https://arch.yonsei.ac.kr/notice/2024-2-%ea%b1...   \n",
       "3   https://arch.yonsei.ac.kr/notice/2024-2%ed%95%...   \n",
       "4   https://arch.yonsei.ac.kr/notice/2024-2%ed%95%...   \n",
       "..                                                ...   \n",
       "75  https://arch.yonsei.ac.kr/notice/2023-2%ed%95%...   \n",
       "76  https://arch.yonsei.ac.kr/notice/2024-1-%ed%86...   \n",
       "77  https://arch.yonsei.ac.kr/notice/2024-1%ed%95%...   \n",
       "78  https://arch.yonsei.ac.kr/notice/2024%ed%95%99...   \n",
       "79  https://arch.yonsei.ac.kr/notice/2023-2-%ed%95...   \n",
       "\n",
       "                                              context  \n",
       "0   2025년 2월 건축공학(4년제) 졸업대상자 Exit Interview 공지졸업 예...  \n",
       "1   IPESK 차세대 공학자 추천 신청 안내 (대학원생 대상)사단법인 한국이공학진흥원(...  \n",
       "2   2024-2 건축공학과 대학원 원우회 정보 공유2024년 11월 15일에 진행된 건...  \n",
       "3   2024-2학기 논문 본심사 상세일정 및 제출서류 안내2024-2학기 논문 본심사를...  \n",
       "4   2024-2학기 석박사 자격시험 신청 안내2024-2학기 석박사 자격시험을 아래와 ...  \n",
       "..                                                ...  \n",
       "75  2023-2학기 학위논문 제출 안내2023학년도 2학기 학위논문 제출시스템이 개편되...  \n",
       "76  2024-1 통합과정 중단 신청 안내2024학년도 1학기 통합과정 중단 신청을 다음...  \n",
       "77  2024-1학기 석사에서 통합으로의 학위과정 변경 신청 안내2024학년도 1학기 석...  \n",
       "78  2024학년도 연세대학교 건축공학과 강사 채용 (23.12.15~12.20)2024...  \n",
       "79  2023-2 학위논문 본심사 상세일정 - 12/18(월) (일정 업데이트)2023-...  \n",
       "\n",
       "[80 rows x 5 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
